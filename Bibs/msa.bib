
@InProceedings{2021.mm.plm.clip,
  title = {Learning Transferable Visual Models From Natural Language Supervision},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  pages = {8748--8763},
  year = {2021},
  volume = {139},
}

@article{2023.mm.plm.survey,
  author = {Xiao Wang and Guangyao Chen and Guangwu Qian and Pengcheng Gao and Xiao{-}Yong Wei and Yaowei Wang and Yonghong Tian and Wen Gao},
  title = {Large-scale Multi-modal Pre-trained Models: {A} Comprehensive Survey},
  journal = {Machine Intelligence Research},
  volume = {20},
  number = {4},
  pages = {447--482},
  year = {2023},
  doi = {https://doi.org/10.1007/s11633-022-1410-8},
}

## MSA Datasets
@article{2016.mm.msa.data-mosi,
  title={Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages},
  author={Zadeh, Amir and Zellers, Rowan and Pincus, Eli and Morency, Louis-Philippe},
  journal={IEEE Intelligent Systems},
  volume={31},
  number={6},
  pages={82--88},
  year={2016},
}
@inproceedings{2018.mm.msa.data-mosei,
  title={Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph},
  author={Zadeh, AmirAli Bagher and Liang, Paul Pu and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  pages={2236--2246},
  year={2018}
}

## MSA Applications
@article{2021.nlp.sa.survey-kbs,
  title = {A comprehensive survey on sentiment analysis: Approaches, challenges and trends},
  journal = {Knowledge-Based Systems},
  volume = {226},
  pages = {107134},
  year = {2021},
  doi = {https://doi.org/10.1016/j.knosys.2021.107134},
  author = {Marouane Birjali and Mohammed Kasri and Abderrahim Beni-Hssane},
}
@article{2022.nlp.sa.survey-air,
  title = {A survey on sentiment analysis methods, applications, and challenges},  
  journal = {Artificial Intelligence Review},  
  author = {Wankhade, Mayur and Rao, Annavarapu Chandra Sekhara and Kulkarni, Chaitanya},  
  year = {2022},  
  pages = {5731–5780},  
  doi = {https://doi.org/10.1007/s10462-022-10144-1}
}
@inproceedings{2022.sa.appl.humancomputer,
  title = {The future of emotion in human-computer interaction},
  author = {Wadley, Greg and Kostakos, Vassilis and Koval, Peter and Smith, Wally and Webber, Sarah and Cox, Anna and Gross, James J and H{\"o}{\"o}k, Kristina and Mandryk, Regan and Slov{\'a}k, Petr},
  booktitle = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
  pages = {1--6},
  year = {2022},
  doi = {https://doi.org/10.1145/3491101.350372}
}
@inproceedings{2021.sa.appl.health,
  title = {Emotion recognition for healthcare surveillance systems using neural networks: A survey},
  author = {Dhuheir, Marwan and Albaseer, Abdullatif and Baccour, Emna and Erbad, Aiman and Abdallah, Mohamed and Hamdi, Mounir},
  booktitle = {2021 International Wireless Communications and Mobile Computing},
  pages = {681--687},
  year = {2021},
  doi = {https://doi.org/10.1109/IWCMC51323.2021.9498861}
}
@inproceedings{2020.sa.appl.robotic,
  title = {A survey of robotics and emotion: Classifications and models of emotional interaction},
  author = {Savery, Richard and Weinberg, Gil},
  booktitle = {2020 29th IEEE International Conference on Robot and Human Interactive Communication},
  pages = {986--993},
  year = {2020},
  doi = {https://doi.org/10.1109/RO-MAN47096.2020.9223536}
}

## MSA Surveys
@article{2024.mm.msa.survey-acs,
  author = {Singh, Upendra and Abhishek, Kumar and Azad, Hiteshwar Kumar},
  title = {A Survey of Cutting-edge Multimodal Sentiment Analysis},
  year = {2024},
  volume = {56},
  number = {9},
  doi = {https://doi.org/https://doi.org/10.1145/3652149},
  journal = {ACM Computing Surveys},
  articleno = {227},
  numpages = {38},
}
@ARTICLE{2019.mm.msa.survey-pami,
  author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title = {Multimodal Machine Learning: A Survey and Taxonomy}, 
  year = {2019},
  volume = {41},
  number = {2},
  pages = {423-443},
  doi = {https://doi.org/10.1109/TPAMI.2018.2798607}
}
@article{2023.mm.msa.survey-if,
  title = {Emotion recognition from unimodal to multimodal analysis: A review},
  journal = {Information Fusion},
  volume = {99},
  pages = {101847},
  year = {2023},
  doi = {https://doi.org/10.1016/j.inffus.2023.101847},
  author = {K. Ezzameli and H. Mahersia},
}
@ARTICLE{2023.mm.msa.survey-tac,
  author = {Poria, Soujanya and Hazarika, Devamanyu and Majumder, Navonil and Mihalcea, Rada},
  journal = {IEEE Transactions on Affective Computing}, 
  title = {Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research}, 
  year = {2023},
  volume = {14},
  number = {1},
  pages = {108-132},
  doi = {https://doi.org/10.1109/TAFFC.2020.3038167}
}
@article{2023.mm.msa.survey-acs,
  author = {Das, Ringki and Singh, Thoudam Doren},
  title = {Multimodal Sentiment Analysis: A Survey of Methods, Trends, and Challenges},
  year = {2023},
  volume = {55},
  number = {13s},
  doi = {https://doi.org/10.1145/3586075},
  journal = {ACM Computing Surveys},
  articleno = {270},
  numpages = {1--38},
}

## MSA Models
@inproceedings{2025.mm.msa.deva,
  title = {Enriching Multimodal Sentiment Analysis through Textual Emotional Descriptions of Visual-Audio Content}, 
  author = {Sheng Wu and Xiaobao Wang and Longbiao Wang and Dongxiao He and Jianwu Dang},
  year = {2025},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence}
}
@inproceedings{2025.mm.msa.dlf,
  title={DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis}, 
  author={Pan Wang and Qiang Zhou and Yawen Wu and Tianlong Chen and Jingtong Hu},
  year = {2025},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence}
}
@article{2023.mm.msa.tetfn,
  title = {TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis},
  journal = {Pattern Recognition},
  volume = {136},
  pages = {109259},
  year = {2023},
  doi = {https://doi.org/10.1016/j.patcog.2022.109259},
  author = {Di Wang and Xutong Guo and Yumin Tian and Jinhui Liu and LiHuo He and Xuemei Luo},
}
@article{2019.mm.msa.cm-gans,
  author = {Peng, Yuxin and Qi, Jinwei},
  title = {CM-GANs: Cross-modal Generative Adversarial Networks for Common Representation Learning},
  year = {2019},
  volume = {15},
  number = {1},
  doi = {https://doi.org/10.1145/3284750},
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  articleno = {22},
  numpages = {24},
}
@inproceedings{2024.mm.msa.hydisgan,
  title = {HyDiscGAN: A Hybrid Distributed cGAN for Audio-Visual Privacy Preservation in Multimodal Sentiment Analysis},
  author = {Wu, Zhuojia and Zhang, Qi and Miao, Duoqian and Yi, Kun and Fan, Wei and Hu, Liang},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence},
  pages = {6550--6558},
  year = {2024},
  doi = {https://doi.org/10.24963/ijcai.2024/724},
}
@inproceedings{2024.mm.msa.kuda,
  title = "Knowledge-Guided Dynamic Modality Attention Fusion Framework for Multimodal Sentiment Analysis",
  author = "Feng, Xinyu and Lin, Yuming and He, Lihua and Li, You and Chang, Liang  and Zhou, Ya",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
  year = "2024",
  doi = "https://doi.org/10.18653/v1/2024.findings-emnlp.865",
  pages = "14755--14766",
}
@article{2025.mm.msa.uefn,
  author = {Shuai Wang and Kuru Ratnavelu and Abdul Samad Bin Shibghatullah},
  title = {{UEFN:} Efficient uncertainty estimation fusion network for reliable multimodal sentiment analysis},
  journal = {Applied Intelligence},
  volume = {55},
  number = {2},
  pages = {171},
  year = {2025},
  doi = {https://doi.org/10.1007/s10489-024-06113-6},
}
@article{2025.mm.msa.atcaf,
  author = {Changqin Huang and Jili Chen and Qionghao Huang and Shijin Wang and Yaxin Tu and Xiaodi Huang},
  title = {AtCAF: Attention-based causality-aware fusion network for multimodal sentiment analysis},
  journal = {Information Fusion},
  volume = {114},
  pages = {102725},
  year = {2025},
  doi = {https://doi.org/10.1016/j.inffus.2024.102725},
}
@article{2025.mm.msa.ulmd,
  title = {Multimodal sentiment analysis with unimodal label generation and modality decomposition},
  journal = {Information Fusion},
  volume = {116},
  pages = {102787},
  year = {2025},
  doi = {https://doi.org/10.1016/j.inffus.2024.102787},
  author = {Linan Zhu and Hongyan Zhao and Zhechao Zhu and Chenwei Zhang and Xiangjie Kong},
}
@article{2025.mm.msa.fcdnet,
  author = {Shuai Liu and Zhe Luo and Weina Fu},
  title = {Fcdnet: Fuzzy Cognition-Based Dynamic Fusion Network for Multimodal Sentiment Analysis},
  journal = {{IEEE} Transactions on Fuzzy System},
  volume = {33},
  number = {1},
  pages = {3--14},
  year = {2025},
  doi = {https://doi.org/10.1109/TFUZZ.2024.3407739},
}
@inproceedings{2022.mm.msa.cubemlp,
  author = {Sun, Hao and Wang, Hongyi and Liu, Jiaqing and Chen, Yen-Wei and Lin, Lanfen},
  title = {CubeMLP: An MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation},
  year = {2022},
  doi = {https://doi.org/10.1145/3503161.3548025},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages = {3722–3729},
  numpages = {8},
}
@inproceedings{2024.mm.msa.glomo,
  author = {Yan Zhuang and Yanru Zhang and Zheng Hu and Xiaoyue Zhang and Jiawen Deng and Fuji Ren},
  title = {GLoMo: Global-Local Modal Fusion for Multimodal Sentiment Analysis},
  booktitle = {Proceedings of the 32nd {ACM} International Conference on Multimedia},
  pages = {1800--1809},
  year = {2024},
  doi = {https://doi.org/10.1145/3664647.3681527},
}
@inproceedings{2013:msa.utterance,
  title={Utterance-level multimodal sentiment analysis},
  author={P{\'e}rez-Rosas, Ver{\'o}nica and Mihalcea, Rada and Morency, Louis-Philippe},
  booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
  pages={973--982},
  year={2013}
}
@inproceedings{2017.mm.msa.tfn,
  title = {Tensor Fusion Network for Multimodal Sentiment Analysis},
  author = {Zadeh, Amir and Chen, Minghai and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  year = {2017},
  pages = {1103--1114},
  doi = {https://doi.org/10.18653/v1/D17-1115}
}
@inproceedings{2018.mm.msa.mfn,
  title = {Memory fusion network for multi-view sequential learning},
  author = {Zadeh, Amir and Liang, Paul Pu and Mazumder, Navonil and Poria, Soujanya and Cambria, Erik and Morency, Louis-Philippe},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume = {32},
  number = {1},
  year = {2018},
  doi = {https://doi.org/10.1609/aaai.v32i1.12021}
}
@inproceedings{2018.mm.msa.lmf,
  title = "Efficient Low-rank Multimodal Fusion With Modality-Specific Factors",
  author = "Liu, Zhun  and Shen, Ying  and Lakshminarasimhan, Varun Bharadhwaj  and Liang, Paul Pu  and Bagher Zadeh, AmirAli  and Morency, Louis-Philippe",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
  year = "2018",
  doi = "https://doi.org/10.18653/v1/P18-1209",
  pages = "2247--2256",
}
@INPROCEEDINGS{2020.mm.msa.deep-hoseq,
  author = {Verma, Sunny and Wang, Jiwei and Ge, Zhefeng and Shen, Rujia and Jin, Fan and Wang, Yang and Chen, Fang and Liu, Wei},
  booktitle = {2020 IEEE International Conference on Data Mining}, 
  title = {Deep-HOSeq: Deep Higher Order Sequence Fusion for Multimodal Sentiment Analysis}, 
  year = {2020},
  pages = {561-570},
  doi = {https://doi.org/10.1109/ICDM50108.2020.00065}
}

@inproceedings{2019:msa.hffn,
  title={Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Affective Computing},
  author={Mai, Sijie and Hu, Haifeng and Xing, Songlong},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  pages={481--492},
}
@inproceedings{2020:msa.iccn,
  title={Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis},
  author={Sun, Zhongkai and Sarma, Prathusha and Sethares, William and Liang, Yingyu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8992--8999},
  year={2020}
}
@inproceedings{2021.mm.msa.tfr-net,
  author = {Yuan, Ziqi and Li, Wei and Xu, Hua and Yu, Wenmeng},
  title = {Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis},
  year = {2021},
  doi = {https://doi.org/10.1145/3474085.3475585},
  booktitle = {Proceedings of the 29th ACM International Conference on Multimedia},
  pages = {4400–4407},
  numpages = {8},
}
@inproceedings{2021.mm.msa.self-mm,
  title = {Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis},
  author = {Yu, Wenmeng and Xu, Hua and Yuan, Ziqi and Wu, Jiele},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume = {35},
  number = {12},
  pages = {10790--10797},
  year = {2021},
  doi = {https://doi.org/10.1609/aaai.v35i12.17289}
}
@inproceedings{2022.mm.msa.chfn,
  author = {Guo, Jiwei and Tang, Jiajia and Dai, Weichen and Ding, Yu and Kong, Wanzeng},
  title = {Dynamically Adjust Word Representations Using Unaligned Multimodal Information},
  year = {2022},
  doi = {https://doi.org/10.1145/3503161.3548137},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages = {3394–3402},
  numpages = {9},
}
@ARTICLE{2022.mm.msa.hycon,
  author = {Mai, Sijie and Zeng, Ying and Zheng, Shuangjia and Hu, Haifeng},
  journal = {IEEE Transactions on Affective Computing}, 
  title = {Hybrid Contrastive Learning of Tri-Modal Representation for Multimodal Sentiment Analysis}, 
  year = {2023},
  volume = {14},
  number = {3},
  pages = {2276-2289},
  doi = {https://doi.org/10.1109/TAFFC.2022.3172360}
}
@inproceedings{2022.mm.msa.hgraph-cl,
  title = "Modeling Intra- and Inter-Modal Relations: Hierarchical Graph Contrastive Learning for Multimodal Sentiment Analysis",
  author = "Lin, Zijie  and Liang, Bin  and Long, Yunfei  and Dang, Yixue  and Yang, Min  and Zhang, Min  and Xu, Ruifeng",
  booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
  year = "2022",
  pages = "7124--7135",
}
@ARTICLE{2024.mm.msa.mcl-mcf,
  author = {Fan, Cunhang and Zhu, Kang and Tao, Jianhua and Yi, Guofeng and Xue, Jun and Lv, Zhao},
  journal = {IEEE Transactions on Affective Computing}, 
  title = {Multi-level Contrastive Learning: Hierarchical Alleviation of Heterogeneity in Multimodal Sentiment Analysis}, 
  year = {2024},
  volume = {},
  number = {},
  pages = {1-17},
  doi = {https://doi.org/10.1109/TAFFC.2024.3423671}
}
@inproceedings{2021.mm.msa.mmim,
  title = "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
  author = "Han, Wei  and Chen, Hui  and Poria, Soujanya",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  year = "2021",
  doi = "https://doi.org/10.18653/v1/2021.emnlp-main.723",
  pages = "9180--9192",
}
@inproceedings{2022.mm.msa.mmcl,
  title = "Multimodal Contrastive Learning via Uni-Modal Coding and Cross-Modal Prediction for Multimodal Sentiment Analysis",
  author = "Lin, Ronghao and Hu, Haifeng",
  booktitle = "Findings of the Association for Computational Linguistics",
  year = "2022",
  doi = "https://doi.org/10.18653/v1/2022.findings-emnlp.36",
  pages = "511--523",
}
@inproceedings{2023.mm.msa.factorcl,
  author = {Paul Pu Liang and Zihao Deng and Martin Q. Ma and James Y. Zou and Louis{-}Philippe Morency and Ruslan Salakhutdinov},
  title = {Factorized Contrastive Learning: Going Beyond Multi-view Redundancy},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2023},
}
@ARTICLE{2023.mm.msa.mlcl,
  author = {Qian, Shengsheng and Xue, Dizhan and Fang, Quan and Xu, Changsheng},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title = {Integrating Multi-Label Contrastive Learning With Dual Adversarial Graph Neural Networks for Cross-Modal Retrieval}, 
  year = {2023},
  volume = {45},
  number = {4},
  pages = {4794-4811},
  doi = {https://doi.org/10.1109/TPAMI.2022.3188547}
}
@inproceedings{2021.mm.msa.mtag,
  title = "{MTAG}: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
  author = "Yang, Jianing and Wang, Yongxin and Yi, Ruitao and Zhu, Yuying  and Rehman, Azaan and Zadeh, Amir and Poria, Soujanya and Morency, Louis-Philippe",
  booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics",
  year = "2021",
  doi = "https://doi.org/10.18653/v1/2021.naacl-main.79",
  pages = "1009--1021",
}
@INPROCEEDINGS{2016.mm.msa.crmkl,
  author = {Poria, Soujanya and Chaturvedi, Iti and Cambria, Erik and Hussain, Amir},
  booktitle = {2016 IEEE 16th International Conference on Data Mining}, 
  title = {Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis}, 
  year = {2016},
  pages = {439-448},
  doi = {https://doi.org/10.1109/ICDM.2016.0055}
}


@inproceedings{2023.mm.msa.confede,
  title = {{C}on{FEDE}: Contrastive Feature Decomposition for Multimodal Sentiment Analysis},
  author = {Yang, Jiuding and Yu, Yakun and Niu, Di and Guo, Weidong and Xu, Yu},
  booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  year = {2023},
  pages = {7617--7630},
  doi = {https://doi.org/10.18653/v1/2023.acl-long.421},
}
@ARTICLE{2024.mm.msa.emt,
  author={Sun, Licai and Lian, Zheng and Liu, Bin and Tao, Jianhua},
  journal={IEEE Transactions on Affective Computing}, 
  title={Efficient Multimodal Transformer With Dual-Level Feature Restoration for Robust Multimodal Sentiment Analysis}, 
  year={2024},
  volume={15},
  number={1},
  pages={309-325},
  doi={https://doi.org/10.1109/TAFFC.2023.3274829}
}

2
## Aligned
@inproceedings{2017.mm.msa.gme-lstm,
  author = {Chen, Minghai and Wang, Sen and Liang, Paul Pu and Baltru\v{s}aitis, Tadas and Zadeh, Amir and Morency, Louis-Philippe},
  title = {Multimodal sentiment analysis with word-level fusion and reinforcement learning},
  year = {2017},
  doi = {https://doi.org/10.1145/3136755.3136801},
  booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  pages = {163–171},
  numpages = {9},
}
@inproceedings{2018.mm.msa.wla,
  title={Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment},
  author={Gu, Yue and Yang, Kangning and Fu, Shiyu and Chen, Shuhong and Li, Xinyu and Marsic, Ivan},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  year={2018},
  pages={2225--2235},
}
@inproceedings{2021.mm.msa.bmf,
  author = {Han, Wei and Chen, Hui and Gelbukh, Alexander and Zadeh, Amir and Morency, Louis-philippe and Poria, Soujanya},
  title = {Bi-Bimodal Modality Fusion for Correlation-Controlled Multimodal Sentiment Analysis},
  year = {2021},
  doi = {https://doi.org/10.1145/3462244.3479919},
  booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
  pages = {6–15},
  numpages = {10},
}
@inproceedings{2019.mm.msa.mtl,
  title = "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis",
  author = "Akhtar, Md Shad  and Chauhan, Dushyant  and Ghosal, Deepanway  and Poria, Soujanya  and Ekbal, Asif  and Bhattacharyya, Pushpak",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
  year = "2019",
  doi = "https://doi.org/10.18653/v1/N19-1034",
  pages = "370--379",
}

@inproceedings{2019.mm.msa.mctn,
  title={Found in translation: Learning robust joint representations by cyclic translations between modalities},
  author={Pham, Hai and Liang, Paul Pu and Manzini, Thomas and Morency, Louis-Philippe and P{\'o}czos, Barnab{\'a}s},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6892--6899},
  year={2019}
}
@inproceedings{2020.mm.msa.argf,
  author = {Sijie Mai and Haifeng Hu and Songlong Xing},
  title = {Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, The Tenth {AAAI} Symposium on Educational Advances in Artificial Intelligence},
  pages = {164--172},
  year = {2020},
  doi = {https://doi.org/10.1609/aaai.v34i01.5347},
}
@inproceedings{2020.mm.msa.transmodality,
  author = {Wang, Zilong and Wan, Zhaohong and Wan, Xiaojun},
  title = {TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis},
  year = {2020},
  doi = {https://doi.org/10.1145/3366423.3380000},
  booktitle = {Proceedings of The Web Conference 2020},
  pages = {2514–2520},
  numpages = {7},
}
@article{2024.mm.msa.dtn,
  author = {Ying Zeng and Wenjun Yan and Sijie Mai and Haifeng Hu},
  title = {Disentanglement Translation Network for multimodal sentiment analysis},
  journal = {Information Fusion},
  volume = {102},
  pages = {102031},
  year = {2024},
  doi = {https://doi.org/10.1016/j.inffus.2023.102031},
}
@inproceedings{2020.mm.msa.mrouting,
  author = {Yao{-}Hung Hubert Tsai and Martin Ma and Muqiao Yang and Ruslan Salakhutdinov and Louis{-}Philippe Morency},
  title = {Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages = {1823--1833},
  year = {2020},
  doi = {https://doi.org/10.18653/v1/2020.emnlp-main.143},
}
@inproceedings{2024.mm.msa.pid,
  author = {Paul Pu Liang and Yun Cheng and Xiang Fan and Chun Kai Ling and Suzanne Nie and Richard J. Chen and Zihao Deng and Nicholas B. Allen and Randy Auerbach and Faisal Mahmood and Russ Salakhutdinov and Louis{-}Philippe Morency},
  title = {Quantifying {\&} Modeling Multimodal Interactions: An Information Decomposition Framework},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2023},
}
@ARTICLE{2023.mm.msa.amml,
  author = {Sun, Ya and Mai, Sijie and Hu, Haifeng},
  journal = {IEEE Transactions on Affective Computing}, 
  title = {Learning to Learn Better Unimodal Representations via Adaptive Multimodal Meta-Learning}, 
  year = {2023},
  volume = {14},
  number = {3},
  pages = {2209-2223},
  doi = {https://doi.org/10.1109/TAFFC.2022.3178231}
}

@inproceedings{2019:msa.mfm,
  title={Learning Factorized Multimodal Representations},
  author={Yao-Hung Hubert Tsai and Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency and Ruslan Salakhutdinov},
  booktitle={International Conference on Learning Representations},
  year={2019},
}
@inproceedings{2019:msa.raven,
  title={Words can shift: Dynamically adjusting word representations using nonverbal behaviors},
  author={Wang, Yansen and Shen, Ying and Liu, Zhun and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={7216--7223},
  year={2019}
}
@inproceedings{2019.mm.msa.mult,
  title = {Multimodal Transformer for Unaligned Multimodal Language Sequences},
  author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J. Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year = {2019},
  pages = {6558--6569},
  doi = {https://doi.org/10.18653/v1/p19-1656},
}
@inproceedings{2016.mm.msa.sgmoe,
  title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  author = {Noam Shazeer and *Azalia Mirhoseini and *Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  booktitle = {International Conference on Learning Representations},
  year = {2017},
}
@inproceedings{2020.mm.msa.emap,
    title = "Does my multimodal model learn cross-modal interactions? It`s harder to tell than you might think!",
    author = "Hessel, Jack and Lee, Lillian",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    year = "2020",
    doi = "https://doi.org/10.18653/v1/2020.emnlp-main.62",
    pages = "861--877",
}
@INPROCEEDINGS{2020.mm.msa.mtf,
  author = {Huang, Jian and Tao, Jianhua and Liu, Bin and Lian, Zheng and Niu, Mingyue},
  booktitle = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
  title = {Multimodal Transformer Fusion for Continuous Emotion Recognition}, 
  year = {2020},
  volume={},
  number={},
  pages = {3507-3511},
  doi = {https://doi.org/10.1109/ICASSP40776.2020.9053762}
}
@inproceedings{2023.mm.msa.almt,
  title = "Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis",
  author = "Zhang, Haoyu  and Wang, Yu and Yin, Guanghao and Liu, Kejun and Liu, Yuanyuan and Yu, Tianshu",
  booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
  year = "2023",
  doi = "https://doi.org/10.18653/v1/2023.emnlp-main.49",
  pages = "756--767",
}

@inproceedings{2020.mm.msa.mag,
  title={Integrating Multimodal Information in Large Pretrained Transformers},
  author={Rahman, Wasifur and Hasan, Md Kamrul and Lee, Sangwu and Bagher Zadeh, AmirAli and Mao, Chengfeng and Morency, Louis-Philippe and Hoque, Ehsan},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  year={2020},
  pages={2359--2369},
  doi = "10.18653/v1/2020.acl-main.214",
}
@inproceedings{2020.mm.msa.misa,
  author = {Hazarika, Devamanyu and Zimmermann, Roger and Poria, Soujanya},
  title = {MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis},
  year = {2020},
  doi = {https://doi.org/10.1145/3394171.3413678},
  booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
  pages = {1122–1131},
  numpages = {10},
}
@InProceedings{2021.mm.msa.pmr,
  author = {Lv, Fengmao and Chen, Xiang and Huang, Yanyong and Duan, Lixin and Lin, Guosheng},
  title = {Progressive Modality Reinforcement for Human Multimodal Emotion Recognition From Unaligned Multimodal Sequences},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2021},
  pages = {2554-2562}
}
@inproceedings{2023.mm.msa.acformer,
  author = {Zong, Daoming and Ding, Chaoyue and Li, Baoxiang and Li, Jiakui and Zheng, Ken and Zhou, Qunyan},
  title = {AcFormer: An Aligned and Compact Transformer for Multimodal Sentiment Analysis},
  year = {2023},
  doi = {https://doi.org/10.1145/3581783.3611974},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  pages = {833–842},
  numpages = {10},
}

## Decoupling
@inproceedings{2021:msa.tcsp,
  title={A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis},
  author={Wu, Yang and Lin, Zijie and Zhao, Yanyan and Qin, Bing and Zhu, Li-Nan},
  booktitle={Findings of the Association for Computational Linguistics},
  year={2021},
  pages={4730--4738},
}
@inproceedings{2022.mm.msa.fdmer,
  author = {Yang, Dingkang and Huang, Shuai and Kuang, Haopeng and Du, Yangtao and Zhang, Lihua},
  title = {Disentangled Representation Learning for Multimodal Emotion Recognition},
  year = {2022},
  doi = {https://doi.org/10.1145/3503161.3547754},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages = {1642–1651},
  numpages = {10},
}
@inproceedings{2022.mm.msa.mfsa,
  author = {Yang, Dingkang and Kuang, Haopeng and Huang, Shuai and Zhang, Lihua},
  title = {Learning Modality-Specific and -Agnostic Representations for Asynchronous Multimodal Language Sequences},
  year = {2022},
  doi = {https://doi.org/10.1145/3503161.3547755},
  booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
  pages = {1708–1717},
  numpages = {10},
}
@InProceedings{2023.mm.msa.ccim,
  author = {Yang, Dingkang and Chen, Zhaoyu and Wang, Yuzheng and Wang, Shunli and Li, Mingcheng and Liu, Siao and Zhao, Xiao and Huang, Shuai and Dong, Zhiyan and Zhai, Peng and Zhang, Lihua},
  title = {Context De-Confounded Emotion Recognition},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2023},
  pages = {19005-19015}
}
@inproceedings{2023.mm.msa.waht2comm,
  author = {Yang, Kun and Yang, Dingkang and Zhang, Jingyu and Wang, Hanqi and Sun, Peng and Song, Liang},
  title = {What2comm: Towards Communication-efficient Collaborative Perception via Feature Decoupling},
  year = {2023},
  doi = {https://doi.org/10.1145/3581783.3611699},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  pages = {7686–7695},
  numpages = {10},
}

@inproceedings{2023.mm.msa.dmd,
  title={Decoupled multimodal distilling for emotion recognition},
  author={Li, Yong and Wang, Yuanzhi and Cui, Zhen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6631--6640},
  year={2023}
}
@InProceedings{2024.mm.msa.corrkd,
  author = {Li, Mingcheng and Yang, Dingkang and Zhao, Xiao and Wang, Shuaibing and Wang, Yan and Yang, Kun and Sun, Mingyang and Kou, Dongliang and Qian, Ziyun and Zhang, Lihua},
  title = {Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2024},
  pages = {12458-12468}
}
@inproceedings{2023:msa.tgsa,
  title={Text-oriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences},
  author={Lei, Yuxuan and Yang, Dingkang and Li, Mingcheng and Wang, Shunli and Chen, Jiawei and Zhang, Lihua},
  booktitle={CAAI International Conference on Artificial Intelligence},
  pages={189--200},
  year={2023},
}
@inproceedings{2023:mer.dferc,
  author = {Li, Bobo and Fei, Hao and Liao, Lizi and Zhao, Yu and Teng, Chong and Chua, Tat-Seng and Ji, Donghong and Li, Fei},
  title = {Revisiting Disentanglement and Fusion on Modality and Context in Conversational Multimodal Emotion Recognition},
  year = {2023},
  booktitle = {Proceedings of the 31st ACM International Conference on Multimedia},
  pages = {5923–5934},
  numpages = {12},
}
@article{2023:msa.tcdn,
  title={Inter-intra modal representation augmentation with trimodal collaborative disentanglement network for multimodal sentiment analysis},
  author={Chen, Chen and Hong, Hansheng and Guo, Jie and Song, Bin},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={31},
  pages={1476--1488},
  year={2023},
}
